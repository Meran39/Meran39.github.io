# 仮想住民シミュレーションプロジェクト

このプロジェクトは、LLM（大規模言語モデル）を完全に委譲して、創造性豊かな仮想住民シミュレーションを実装することを目的としています。小規模なエージェント数（20〜50 体）から開始し、長期運用（週次/月次ステップ）も可能なアーキテクチャを検証します。

## フェーズ 1：LLM 単一エージェント呼び出し

### 実装内容

- **Agent クラスの実装**: `id` と `memory` プロパティを持つエージェントクラスを定義しました。
- **Hugging Face Inference API 呼び出しラッパーの作成**: LLM サービスと連携するためのラッパー（`LLMService`）を実装しました。これにより、Hugging Face Inference API を介してエージェントの行動を生成します。
- **プロンプト設計**: システムメッセージとユーザーメッセージを定義し、エージェントの行動決定に利用しています。
- **単一エージェントの挙動確認**: テストスクリプト（またはアプリケーションの初期状態）で、単一エージェントが LLM を呼び出して行動を生成できることを確認しました。

### 解決したエラー

- **エラー内容**: `src/App.tsx` にて「プロパティ 'env' は型 'ImportMeta' に存在しません。」という TypeScript エラーが発生しました。これは、`import.meta.env` へのアクセスに関する型定義の問題でした。
- **解決策**: `vite-env.d.ts` ファイルが正しく `ImportMetaEnv` インターフェースと `ImportMeta` を定義していること、および `tsconfig.json` に `vite-env.d.ts` が含まれていることを確認しました。TypeScript コンパイラ (`tsc --noEmit`) ではエラーが報告されなかったため、IDE の言語サーバーのキャッシュ問題であると判断しました。IDE の再起動、または関連ファイルの再保存で解決する可能性が高いことを確認しました。

---

## フェーズ 2：複数エージェントの導入と相互作用

### 実装内容

- **複数エージェントの管理**: シミュレーション内に複数のエージェントを生成・管理する機能を実装します。
- **基本的な相互作用**: エージェント同士が互いを認識し、簡単なコミュニケーション（挨拶など）を行えるようにします。
- **シミュレーションループ**: 各エージェントが順番に行動を選択し、実行する基本的なシミュレーションのメインループを構築します。

## フェーズ 3：環境との相互作用と状態変化

### 実装内容

- **環境（場所）の概念の導入**: エージェントが存在する場所（家、職場、公園など）を定義し、移動できるようにします。
- **環境とのインタラクション**: エージェントが環境内のオブジェクトに対して行動（例：PC を使う、コーヒーを飲む）できるようにします。
- **エージェントの状態管理**: エージェントの基本的な状態（例：幸福度、所持金）を導入し、行動によって変化するようにします。

## フェーズ 4：記憶と学習

### 実装内容

- **長期記憶の実装**: エージェントが過去の重要な出来事や情報を記憶し、後で参照できる仕組みを `memoryManager.ts` を中心に構築します。
- **記憶に基づく行動変化**: 過去の記憶（例：誰と会ったか、何をしたか）が、エージェントの将来の行動決定に影響を与えるようにします。
- **忘却のメカニズム**: 重要度の低い記憶を時間経過とともに忘れる、より人間に近い記憶モデルを検討・実装します。

## フェーズ 5：目標と計画の立案

### 実装内容

- **長短期の目標設定**: エージェントに「スキルを習得する」「友人を増やす」といった長期的な目標と、それに基づいた短期的な目標（今日のタスク）を与えます。
- **計画の自動生成**: `decisionManager.ts` を活用し、LLM にエージェントの目標達成のための行動計画を立案させます。
- **計画の実行と再評価**: エージェントは計画に沿って行動し、状況の変化に応じて計画を動的に見直します。

## フェーズ 6：経済システムの導入

### 実装内容

- **職業と収入**: エージェントが職業に就き、労働によって収入を得る仕組みを導入します。
- **消費活動**: `costOptimizer.ts` を発展させ、エージェントが収入を使って物やサービスを購入する消費活動をシミュレートします。
- **基本的な経済モデル**: シンプルな需要と供給の概念を取り入れ、物価などが変動する基礎的な経済システムを構築します。

## フェーズ 7：社会システムの導入とルールの適用

### 実装内容

- **社会的な関係性の深化**: 友人、家族、同僚といった、より複雑な人間関係の概念を導入します。
- **社会規範とルールの実装**: `ruleEngine.ts` を用いて、シミュレーション世界内の社会的なルールを定義し、エージェントがそれに従うようにします。
- **コミュニティの形成**: エージェント同士の相互作用を通じて、自然発生的にグループやコミュニティが形成されるプロセスをシミュレートします。

## フェーズ 8：シミュレーションの長期運用と可視化

### 実装内容

- **長期運用アーキテクチャ**: シミュレーションを週次・月次といった長期的な時間ステップで安定して実行できるアーキテクチャを検証・構築します。
- **高度な可視化**: `AgentVisualization.tsx` や `SimulationLogs.tsx` を強化し、エージェントの行動履歴、人間関係、社会の発展などを直感的に理解できるダッシュボードを開発します。
- **ユーザー介入機能**: ユーザーがシミュレーションに影響を与える（例：新しい住民を追加する、経済政策を実施する）機能を実装します。

---

### 現在のプロジェクト概要 (2025 年 7 月 25 日)

- **シミュレーションの基盤**: LLM を活用した仮想住民シミュレーションの基本的なフレームワークが構築されています。単一および複数エージェントの行動決定、相互作用、環境との基本的なインタラクションが可能です。
- **ゾンビシミュレーションのプロトタイプ**: ゾンビの出現、移動、エージェントや拠点との衝突・ダメージ処理が実装されています。ゾンビの描画と挙動に関するデバッグと改善を行いました。
- **エージェントの行動と状態**: エージェントは LLM によって行動を決定し、幸福度、空腹度、エネルギーなどの状態が変化します。武器の所持や物資調達の概念も導入されています。
- **可視化**: D3.js を用いた 2D マップ形式でのエージェント、場所、ゾンビの可視化が実装されています。
- **主な改善点**:
  - `App.tsx`における`toggleRunning`および`resetSimulation`関数の重複定義を解消。
  - `decisionManager.ts`内の`moveRegex`を修正し、LLM からの`targetLocation`のパース精度を向上。
  - `App.tsx`のゾンビ生成位置をマップの表示範囲内に調整。
  - `AgentVisualization.tsx`にてゾンビの描画設定（サイズ、色）を一時的に変更し、視認性を向上。
  - `App.tsx`にゾンビの生成座標と`zombies`配列の内容をログ出力するデバッグコードを追加。
  - ゾンビの初期体力を`100`から`500`に増加させ、マップ上での生存時間を延長。
  - `App.tsx`におけるゾンビの移動とダメージ処理のロジックを調整し、ゾンビが生成直後に消滅する問題を修正。
  - `App.tsx`のゾンビ移動ロジックにおける`prevZombies`の参照エラーを修正。

### 技術スタックの選定理由

- **React**: UI 構築のための宣言的で効率的なライブラリ。コンポーネントベースのアプローチにより、複雑な UI を管理しやすく、再利用性を高めます。
- **TypeScript**: 静的型付けにより、開発中のエラーを早期に発見し、コードの品質と保守性を向上させます。大規模なプロジェクトでの開発効率を高めます。
- **D3.js**: データ駆動型ドキュメントの作成に特化しており、複雑なデータ（エージェントの位置、ゾンビの動きなど）を SVG で柔軟かつ高性能に可視化できます。
- **Node.js/Express.js**: バックエンド API の構築に利用。LLM との連携やデータ処理を効率的に行うための軽量で高速な環境を提供します。
- **Vite**: 高速な開発サーバーとビルドプロセスを提供し、開発体験を向上させます。
- **Ollama (ローカル LLM)**:
  - **プライバシーとセキュリティ**: 外部 API にデータを送信することなく、ローカル環境で LLM を実行できるため、機密性の高いシミュレーションデータも安全に扱えます。
  - **コスト効率**: API 利用料を気にすることなく、大量の LLM 呼び出しを自由に実行できます。
  - **カスタマイズ性**: 必要に応じてモデルを微調整したり、独自のモデルをデプロイしたりする柔軟性があります。
  - **オフライン動作**: インターネット接続がない環境でもシミュレーションを実行できます。

### ローカル LLM のセットアップ手順

1.  **Ollama のインストール**:
    - Ollama の公式サイト (https://ollama.com/) から、お使いの OS に合ったインストーラーをダウンロードし、インストールしてください。
2.  **モデルのダウンロード**:
    - ターミナルを開き、以下のコマンドで必要なモデルをダウンロードします。
      ```bash
      ollama pull llama2
      ```
      （`llama2`はデフォルトモデルですが、`VITE_OLLAMA_MODEL`で指定したモデルに合わせてください。）
3.  **Vite 環境変数の設定**:
    - プロジェクトのルートディレクトリに`.env`ファイルを作成し、Ollama のベース URL を設定します。
      ```
      VITE_OLLAMA_BASE_URL=http://localhost:11434
      VITE_DEFAULT_MODEL=llama2
      ```
      （Ollama がデフォルトポートで動作している場合。異なる場合は適宜変更してください。）
4.  **Ollama サーバーの起動**:
    - Ollama をインストールすると、通常はバックグラウンドで自動的にサーバーが起動します。もし起動していない場合は、以下のコマンドで手動で起動できます。
      ```bash
      ollama run llama2
      ```
      （このコマンドはモデルをダウンロードし、サーバーを起動します。）

### 今後の展望

- **LLM プロンプトのさらなる洗練**: エージェントの行動がより複雑で現実的になるよう、プロンプトエンジニアリングを継続的に改善。
- **シミュレーションのパフォーマンス最適化**: 大規模なエージェント数や長期運用に対応するため、レンダリングや LLM 呼び出しの効率化。
- **ユーザーインターフェースの強化**: ユーザーがシミュレーションのパラメータを動的に変更したり、特定のイベントをトリガーしたりできるインタラクティブな UI の追加。
- **新しい LLM モデルの実験**: より高性能なローカル LLM や、特定のタスクに特化したモデルの導入を検討。
- **より複雑な社会・経済システムの導入**: 中立や敵対生存者などの追加。
- **時間の導入、滑らかなアニメーションの導入**:時間の視覚化とアニメーションの導入を検討。
